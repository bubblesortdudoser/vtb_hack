{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\misha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\misha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\misha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\misha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\misha\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance as lev_distance\n",
    "from collections import Counter, defaultdict\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sns.set(context='notebook', style='whitegrid', palette='pastel', \n",
    "        font='sans-serif', font_scale=1, color_codes=False, rc=None)\n",
    "\n",
    "from nltk import pos_tag, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('all')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('interfax_2.csv')\n",
    "text = ''\n",
    "for i in range(data.shape[0]):\n",
    "    data.loc[i, 'text'] = data.loc[i, 'text'].lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punkt_list = ['.', ',', '/', '`', '\"', \"'\", '!', '&', '?', '(', ')', '-', '+', '_', '*', '@', ';', ':', '<',\n",
    "         '>', '\\\\', '[', ']', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '“', '„', '%', '$', '…',\n",
    "         '’', '|', '^', '~', '=', '»', '”', '′', '•', '—', '«', '–']\n",
    "\n",
    "flag = True\n",
    "for i in range(len(punkt_list)):\n",
    "    for j in range(i+1, len(punkt_list)):\n",
    "        if punkt_list[i] == punkt_list[j]:\n",
    "            print(f'1й индекс: {i}, 2й индекс: {j}, символ: {punkt_list[i]}')\n",
    "            flag = False\n",
    "if flag:\n",
    "    print('В списке \"punkt_list\" нет повторяющихся элементов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = stopwords.words('russian') + stopwords.words('english')\n",
    "rubbish_list = ['которые', 'нам', 'дали', 'например', 'пока', 'часть', 'лишь', 'это', 'также', 'будут',\n",
    "                'россии', 'россия', 'года', 'годы', 'году', 'год', 'январь', 'января', 'февраля', 'могут',\n",
    "                'февраль', 'март', 'марта', 'апрель', 'апреля', 'май', 'мая', 'июнь', 'июня', 'июль',\n",
    "                'июля', 'август', 'августа', 'сентябрь', 'сентября', 'октябрь', 'октября', 'ноябрь',\n",
    "                'ноября', 'декабрь', 'декабря', 'москва', 'interfaxru', 'сша', 'рф', 'млн', 'млрд', 'тыс',\n",
    "                'рубль', 'рублей', 'рубля', 'рублю', 'сентябре', 'январе', 'феврале', 'марте', 'апреле',\n",
    "                'мае', 'июне', 'июле', 'августе', 'октябре', 'ноябре', 'декабре', 'говорится', 'ранее',\n",
    "                'изза', 'когдалибо', 'когдато', 'гделибо', 'гдето', 'московской', 'сообщил', 'составил',\n",
    "                'новых', 'новые', 'понедельник', 'вторник', 'среду', 'четверг', 'пятницу', 'субботу',\n",
    "                'воскресенье', 'рост', 'роста', 'фоне', 'итогам', 'итоги', 'время', 'стран', 'страна',\n",
    "                'который', 'словам', 'начала', 'сранению', 'заявил', 'руб', 'числе', 'сообщила', 'решение',\n",
    "                'страны', 'тысяч', 'конце', 'около', 'сказал', 'российских', 'однако', 'данным', 'дня',\n",
    "                'сообщении', 'относительно', 'возможность', 'пишет', 'времени', 'период', 'которых',\n",
    "                'неделе', 'российской', 'рамках', 'отмечает', 'поскольку', 'месяцев', 'отметил', 'ходе',\n",
    "                'срок', 'тонн', 'является', 'сообщает', 'российского', 'которая', 'составила', 'открылся',\n",
    "                'российские', 'количество', 'впервые']\n",
    "stop_list += rubbish_list\n",
    "\n",
    "flag = True\n",
    "for i in range(len(stop_list)):\n",
    "    for j in range(i+1, len(stop_list)):\n",
    "        if stop_list[i] == stop_list[j]:\n",
    "            print(f'1й индекс: {i}, 2й индекс: {j}, символ: {stop_list[i]}')\n",
    "            flag = False\n",
    "if flag:\n",
    "    print('В списке \"stop_list\" нет повторяющихся элементов')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(data.shape[0]):\n",
    "    string = data.loc[i, 'text']\n",
    "    for sign in punkt_list:\n",
    "        string = string.replace(sign, '') # чистка знаков препинания\n",
    "    string = string.replace('\\n', ' ') # чистка переносов строки\n",
    "    string = string.replace('\\t', ' ') # чистка табов\n",
    "    string = string.replace('  ', ' ') # удаление двойных пробелов\n",
    "    string = word_tokenize(string) # токенизация по словам\n",
    "    string = [word for word in string if not word in stop_list] # удаление стоп-слов\n",
    "    data.loc[i, 'text'] = ''\n",
    "    for j in string:\n",
    "        data.loc[i, 'text'] += ' '+j\n",
    "    data.loc[i, 'text'] = data.loc[i, 'text'][1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data['text']:\n",
    "    text += ' '+i\n",
    "for sign in punkt_list:\n",
    "    text = text.replace(sign, '') \n",
    "text = text.replace('\\n', ' ') \n",
    "text = text.replace('\\t', ' ') \n",
    "text = text.replace('  ', ' ')\n",
    "text = word_tokenize(text)\n",
    "text = [word for word in text if not word in stop_list]\n",
    "sorted(Counter(text).items(), key=lambda x: x[1], reverse=True)[:30] # частотность слов (настройка стоп-слов)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Стемминг и лемматизация в данном случае только затрудняли работу с текстом (возможно, из-за того, что библиотечные методы не так хорошо работают с русским языком)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    text_list.append(data.loc[i, 'text'])\n",
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
